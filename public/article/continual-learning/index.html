<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>üìù Continual Learning: Introduction | Sirius14 Blog</title>
<meta name=keywords content="AI,Article,Pytorch,Python,Continual Learning">
<meta name=description content="Accelerating AI/ML Workloads with continual learning">
<meta name=author content="Sirius14">
<link rel=canonical href=https://xelon-coder.github.io/Sirius14_Blog/article/continual-learning/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/Sirius14_Blog/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style>
<link rel=icon href=https://xelon-coder.github.io/Sirius14_Blog/img/logo.png>
<link rel=icon type=image/png sizes=16x16 href=https://xelon-coder.github.io/Sirius14_Blog/img/logo.png>
<link rel=icon type=image/png sizes=32x32 href=https://xelon-coder.github.io/Sirius14_Blog/img/logo.png>
<link rel=apple-touch-icon href=https://xelon-coder.github.io/Sirius14_Blog/img/logo.png>
<link rel=mask-icon href=https://xelon-coder.github.io/Sirius14_Blog/img/logo.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="üìù Continual Learning: Introduction">
<meta property="og:description" content="Accelerating AI/ML Workloads with continual learning">
<meta property="og:type" content="article">
<meta property="og:url" content="https://xelon-coder.github.io/Sirius14_Blog/article/continual-learning/"><meta property="og:image" content="https://xelon-coder.github.io/Sirius14_Blog/img/logo.png"><meta property="article:section" content="article">
<meta property="article:published_time" content="2024-01-29T14:46:56+01:00">
<meta property="article:modified_time" content="2024-01-29T14:46:56+01:00"><meta property="og:site_name" content="Sirius14 Blog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://xelon-coder.github.io/Sirius14_Blog/img/logo.png">
<meta name=twitter:title content="üìù Continual Learning: Introduction">
<meta name=twitter:description content="Accelerating AI/ML Workloads with continual learning">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Articles","item":"https://xelon-coder.github.io/Sirius14_Blog/article/"},{"@type":"ListItem","position":2,"name":"üìù Continual Learning: Introduction","item":"https://xelon-coder.github.io/Sirius14_Blog/article/continual-learning/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"üìù Continual Learning: Introduction","name":"üìù Continual Learning: Introduction","description":"Accelerating AI/ML Workloads with continual learning","keywords":["AI","Article","Pytorch","Python","Continual Learning"],"articleBody":"This article is a summary of continual learning algorithms explaining how it could be a game changer over the next few years.\nIntro Artificial intelligence (AI) is a vast domain which covers plenty of usage and various needs. Some sectors as intelligent cars are quickly evolving with the emergence of new heavy datasets and new training data which enable to improve the prediction of the model. In this domain, predictions need to be precise, indeed cyclist should not be recognize as moto otherwise it could be difficult for the model to choose the good decision and it may cause accidents. With the aim of improving the prediction, we need to constantly relearn with new data, a new example of the situation.\nHowever, even though these deep learning (DL) models have better results, training time is just increasing due to a new complex datasets. Moreover, prediction could be enhanced with dataset generated in real time. Considering this fact DL models are not conceived to be trained with continual new data, they need to train from scratch with only one big task containing all the datasets. Consequently, finding a model which enables to learn continuously and to accelerate the learning process. Incremental learning has been implemented, nevertheless, utilizing CL to acquire new data results in catastrophic forgetting. Naturally, as the model learns new data, it endeavors to center its predictions around the newly provided data. DL doesn‚Äôt have this problem because it learns from scratch each time and concatenates the new data with the previous ones. Thus, it could be interesting to find How to accelerate learning process and avoid catastrophic forgetting?\nContinual Learning Before explaining what does involve continual learning (CL), I need to introduce some vocabulary. A task includes several classes, in DL case we only have one big task. On the different point of view, CL using the same models, structures, and goal. However, instead of learning one unique task as DL, we want to learn a set of tasks. The goal is to split learning in several tasks.\nFigure 1: Schema explaining catastrophic forgetting problem\nThis simple variation in DL system changes a lot the results. During the first task no major difference with DL, but when the after the second task we evaluate the model, we encounter a catastrophic forgetting (CF), and this is the most problematic thing in CL. Indeed, CF exist because during a task the parameters will be moving toward a local minimum of the loss function. Furthermore, when the model begins to learn a new task, this local minimum will be moved to another part since input data are not the same. This is represented on Figure 1, red cross is the local minimum, the blue arrow is the transition of the minimum between 2 tasks. The second part of the figure shows that for the classification of classes belongs to the preceding task, the local minimum is no longer center so the classification will be worse. This behaviour is what we want to avoid, this is a crucial point in CL research, and this is why nowadays we need to use a DL model.\nHaving a learning process separated in different tasks each one containing class can reduce the time to learn the model, in addition with distributed workflows it will be better. Thus, if we want to use CL, we need algorithms which avoid CF, our goal is to bring closer DL results.\nState of Art There are two main approaches to conceive CL algorithm:\n  Rehearsal: this method is a very common way of thinking, if the class of the first task has bad predictions, we can create a buffer which can be added to the input data. This buffer gives older data from previous tasks, to sum up this method consists of store raw data samples in a buffer, to be replayed when training on a new task.\n  Regularisation: this approach is based on avoiding overwriting the knowledge previously acquired in adding some constraint to guide the model on the weights, on the probabilities, on the gradients, on the features.\n  It exists other method but, in this internship, I only use a rehearsal or regularisation or a blend of them. So, I begin to study some scientific papers on these types of algorithms, this paper was very recently from 2017 to 2021. Each one detail the how does it work with numbers and compares to each other, however it was hard to have the same result on the same datasets, conditions of experimentation always change and it was complicated to find algorithms better than the other.\nExperience Replay (ER) So as Figure 3, show this implementation consist of concatenated the actual train mini batch (batch form the current task) with a buffer mini batch. For each new task, we store in the buffer some representatives of previous class from the previous task. This percentage can be changed to add more diverse example, this is possible to write the number of buffers mini batch which is concatenated. This algorithm was like a baseline for me to evaluate performance and assess the potential for improvement over this version.\nFigure 3: Er schema\nAveraged Gradient Episodic Memory (A-GEM) This time, this is a new algorithm in its methods, A-GEM is a regularization based so the model update rule is modified. Even though we can see a buffer in Figure 4, his role, his just to guide the model and not to train. It begins like DL, the model is trained on the current mini batch Mtask (or NNxy), then a copy of this model is trained on some representatives from the buffer, and we called this model Mbuffer (or NNrb). With these two models, we perform a scalar product in order to view if there are similar, to know if the knowledge doesn‚Äôt disappear.With a negative scalar product, we keep the current task, otherwise we make a projection of the Mtask model on Mbuffer. So this time buffer is used for evaluating and correct the update current model. To compare with ER model, we have:\n  Constraint when updating the parameters of the Mtask, mitigating catastrophic forgetting.\n  No proper rehearsal but a rehearsal buffer is still needed to obtain Mbuffer.\n  Figure 4: Agem schema\nHindsight Anchor Learning (HAL) This approach seems to be relatively good, resembling an enhancement of ER. This algorithm is both rehearsal and regularization based, indeed we keep ER implementation with the buffer, but we are adding a constraint during update of the model. We want to limit the forgetting on historical data points representative of a class (called anchors).For each class of each task, we have one anchor, it encodes points that would maximize the catastrophic forgetting. Thanks to the anchors, the model cannot diverge too much using this method. These are used in the update calculation and are calculated with at the end of each task. Creating anchors is linear because you don‚Äôt need to re-calculate them, nevertheless all the anchors are used to update the model. If we compare to ER contrary to A-Gem, we have also similarities:\n  System of anchors differs to ER and bring the regularization method to this algorithm.\n  The buffer is still concatenated to the current task buffer so the same thing as ER.\n  Figure 5: Hal schema\nDark Experience Replay and improvement (DER/DER++) Before explaining the how it works, I need to briefly explain what knowledge distillation is.\nKnowledge distillation (KD) KD is separated in two models, teacher and student. The student model delivers predictions. There are 2 kinds of predictions:\n  Soft predictions are a tensor of probabilities also called lofts, in fact the last layer of our model, the step before we predict the class.\n  Hard predictions are the step after, the class predicted by our model.\n  Figure 6: Knowledge distillation schema\nThen we can calculate the loss of each one of these predictions, we combine distillation loss and student loss into an only loss value using coefficient to regulate if we prefer to highlight one loss compared to the other ones. As you can observe in Figure 6 we have a green circle and a red circle, this means that DER algorithm only used distillation loss and not combine to student loss in green. In red we have DER++ which add student loss.\nDER This algorithm is rehearsal and regularization based. It complements rehearsal with an additional knowledge distillation objective, to the difference between ER is that we store logits (soft labels) alongside its representative in the buffer instead of relying on a single class output (hard label). Thus, we need to modify the actual buffer implementation (which is the buffer‚Äôs one). Regularization aspect is brought by distillation loss that we add to current mini batch loss. We still have difference with an ER model like:\n  Representatives are stored alongside their logits (activations of last layer).\n  Evaluate the model on both older logits and current ones (knowledge distillation).\n  Figure 7: Der schema\nDER++ DER++ This is an improvement of DER, this algorithm is still rehearsal and regularization based, in a nutshell, ER uses hard labels only (class outputs), DER uses soft labels only (logits). DER++ uses both. Conceptually nothing changes between DER and DER++ except the buffer implementation as a matter of the fact we need to store labels and logits, so in contrary of DER we store an additional component, which leave us less size for representatives.\nFigure 8: Der++ schema\nConclusion Continual Learning algorithms concept is still young and Machine learning algorithms continue to outperform them in terms of performance. However usage is not the same, and may be in 10 years catastrophic forgetting will no longer be an issue. Then CL could be very useful in our daily lives. This area of research is vast and each year new algorithms arrive, but we are still in the early stages. Over this report I explore the implementation and deployment part of a distributed testbed. The impact of these presented algorithms is still underperforming against DL but with parallelisation of calculations and a learning process which don‚Äôt incrementally this type of machine learning could have a real impact on domain like medicine, finance, or autonomous car. With a faster training phase and incremental learning, it‚Äôs also cheaper, and we can create new data in real time to improve the model without training from scratch.\nSource Every explaination of this article was based on research paper and represent the culmination of an internship I had the opportunity to undertake. The result I obtained as well as implementation are not available in this blog. If you want more info you can DM me in Discord.\nER : https://arxiv.org/pdf/1811.11682.pdf\nA-GEM : https://arxiv.org/pdf/1812.00420.pdf\nHAL : https://arxiv.org/pdf/2002.08165.pdf\nDER : https://arxiv.org/pdf/2004.07211.pdf\nDER ++ : https://arxiv.org/pdf/2201.00766.pdf\nContinual Learning Pytorch Framework : https://github.com/aimagelab/mammoth\n","wordCount":"1795","inLanguage":"en","datePublished":"2024-01-29T14:46:56+01:00","dateModified":"2024-01-29T14:46:56+01:00","author":{"@type":"Person","name":"Sirius14"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xelon-coder.github.io/Sirius14_Blog/article/continual-learning/"},"publisher":{"@type":"Organization","name":"Sirius14 Blog","logo":{"@type":"ImageObject","url":"https://xelon-coder.github.io/Sirius14_Blog/img/logo.png"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://xelon-coder.github.io/Sirius14_Blog/ accesskey=h title="Home (Alt + H)">
<img src=https://xelon-coder.github.io/Sirius14_Blog/img/logo.png alt aria-label=logo height=35>Home</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://xelon-coder.github.io/Sirius14_Blog/writeups/ title=Writeups>
<span>Writeups</span>
</a>
</li>
<li>
<a href=https://xelon-coder.github.io/Sirius14_Blog/article/ title=Article>
<span>Article</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class="post-title entry-hint-parent">
üìù Continual Learning: Introduction
</h1>
<div class=post-description>
Accelerating AI/ML Workloads with continual learning
</div>
<div class=post-meta><span title="2024-01-29 14:46:56 +0100 CET">January 29, 2024</span>&nbsp;¬∑&nbsp;9 min&nbsp;¬∑&nbsp;Sirius14&nbsp;|&nbsp;<a href=https://xelon-coder.github.io/Sirius14_Blog//article/continual-learning.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><nav id=TableOfContents>
<ul>
<li><a href=#intro>Intro</a></li>
<li><a href=#continual-learning>Continual Learning</a></li>
<li><a href=#state-of-art>State of Art</a>
<ul>
<li><a href=#experience-replay-erhttpsarxivorgpdf181111682pdf>Experience Replay (<a href=https://arxiv.org/pdf/1811.11682.pdf>ER</a>)</a></li>
<li><a href=#averaged-gradient-episodic-memory-a-gemhttpsarxivorgpdf181200420pdf>Averaged Gradient Episodic Memory (<a href=https://arxiv.org/pdf/1812.00420.pdf>A-GEM</a>)</a></li>
<li><a href=#hindsight-anchor-learning-halhttpsarxivorgpdf200208165pdf>Hindsight Anchor Learning (<a href=https://arxiv.org/pdf/2002.08165.pdf>HAL</a>)</a></li>
<li><a href=#dark-experience-replay-and-improvement-derhttpsarxivorgpdf200407211pdfderhttpsarxivorgpdf220100766pdf>Dark Experience Replay and improvement (<a href=https://arxiv.org/pdf/2004.07211.pdf>DER</a>/<a href=https://arxiv.org/pdf/2201.00766.pdf>DER++</a>)</a></li>
</ul>
</li>
<li><a href=#conclusion>Conclusion</a></li>
<li><a href=#source>Source</a></li>
</ul>
</nav>
</div>
</details>
</div>
<div class=post-content><p>This article is a summary of continual learning algorithms explaining how it could be a game changer over the next few years.</p>
<h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2>
<p>Artificial intelligence (AI) is a vast domain which covers plenty of usage and various needs. Some
sectors as intelligent cars are quickly evolving with the emergence of new heavy datasets and new
training data which enable to improve the prediction of the model. In this domain, predictions need to
be precise, indeed cyclist should not be recognize as moto otherwise it could be difficult for the model
to choose the good decision and it may cause accidents. With the aim of improving the prediction, we
need to constantly relearn with new data, a new example of the situation.</p>
<p>However, even though these deep learning (DL) models have better results, training time is just
increasing due to a new complex datasets. Moreover, prediction could be enhanced with dataset generated in real time. Considering this fact DL models are not conceived to be trained with continual
new data, they need to train from scratch with only one big task containing all the datasets. Consequently, finding a model which enables to learn continuously and to accelerate the learning process.
Incremental learning has been implemented, nevertheless, utilizing CL to acquire new data results in
catastrophic forgetting. Naturally, as the model learns new data, it endeavors to center its predictions
around the newly provided data. DL doesn‚Äôt have this problem because it learns from scratch each
time and concatenates the new data with the previous ones. Thus, it could be interesting to find How
to accelerate learning process and avoid catastrophic forgetting?</p>
<h2 id=continual-learning>Continual Learning<a hidden class=anchor aria-hidden=true href=#continual-learning>#</a></h2>
<p>Before explaining what does involve continual learning (CL), I need to introduce some vocabulary.
A task includes several classes, in DL case we only have one big task. On the different point of view,
CL using the same models, structures, and goal. However, instead of learning one unique task as DL,
we want to learn a set of tasks. The goal is to split learning in several tasks.</p>
<p><img loading=lazy src=/Sirius14_Blog/img/article/cl_1.png alt="Catastrophic Forgetting">
<em>Figure 1: Schema explaining catastrophic forgetting problem</em></p>
<p>This simple variation in DL system changes a lot the results. During the first task no major difference with DL, but when the after the second task we evaluate the model, we encounter a catastrophic
forgetting (CF), and this is the most problematic thing in CL. Indeed, CF exist because during a task
the parameters will be moving toward a local minimum of the loss function. Furthermore, when the
model begins to learn a new task, this local minimum will be moved to another part since input data
are not the same. This is represented on Figure 1, red cross is the local minimum, the blue arrow is the transition of the minimum between 2 tasks. The second part of the figure shows that for the
classification of classes belongs to the preceding task, the local minimum is no longer center so the
classification will be worse. This behaviour is what we want to avoid, this is a crucial point in CL
research, and this is why nowadays we need to use a DL model.</p>
<p>Having a learning process separated in different tasks each one containing class can reduce the time
to learn the model, in addition with distributed workflows it will be better. Thus, if we want to use
CL, we need algorithms which avoid CF, our goal is to bring closer DL results.</p>
<h2 id=state-of-art>State of Art<a hidden class=anchor aria-hidden=true href=#state-of-art>#</a></h2>
<p>There are two main approaches to conceive CL algorithm:</p>
<ul>
<li>
<p>Rehearsal: this method is a very common way of thinking, if the class of the first task has bad
predictions, we can create a buffer which can be added to the input data. This buffer gives older
data from previous tasks, to sum up this method consists of store raw data samples in a buffer,
to be replayed when training on a new task.</p>
</li>
<li>
<p>Regularisation: this approach is based on avoiding overwriting the knowledge previously acquired in adding some constraint to guide the model on the weights, on the probabilities, on the
gradients, on the features.</p>
</li>
</ul>
<p>It exists other method but, in this internship, I only use a rehearsal or regularisation or a blend of
them. So, I begin to study some scientific papers on these types of algorithms, this paper was very
recently from 2017 to 2021. Each one detail the how does it work with numbers and compares to each
other, however it was hard to have the same result on the same datasets, conditions of experimentation
always change and it was complicated to find algorithms better than the other.</p>
<h3 id=experience-replay-erhttpsarxivorgpdf181111682pdf>Experience Replay (<a href=https://arxiv.org/pdf/1811.11682.pdf>ER</a>)<a hidden class=anchor aria-hidden=true href=#experience-replay-erhttpsarxivorgpdf181111682pdf>#</a></h3>
<p>So as Figure 3, show this implementation consist of concatenated the actual train mini batch (batch form the current task) with
a buffer mini batch. For each new task, we store in the buffer some representatives of previous class
from the previous task. This percentage can be changed to add more diverse example, this is possible
to write the number of buffers mini batch which is concatenated. This algorithm was like a baseline
for me to evaluate performance and assess the potential for improvement over this version.</p>
<p><img loading=lazy src=/Sirius14_Blog/img/article/cl_3.png alt=Er>
<em>Figure 3: Er schema</em></p>
<h3 id=averaged-gradient-episodic-memory-a-gemhttpsarxivorgpdf181200420pdf>Averaged Gradient Episodic Memory (<a href=https://arxiv.org/pdf/1812.00420.pdf>A-GEM</a>)<a hidden class=anchor aria-hidden=true href=#averaged-gradient-episodic-memory-a-gemhttpsarxivorgpdf181200420pdf>#</a></h3>
<p>This time, this is a new algorithm in its methods, A-GEM is a regularization based so the model
update rule is modified. Even though we can see a buffer in Figure 4, his role, his just to guide the model and not to train.
It begins like DL, the model is trained on the current mini batch Mtask (or
NNxy), then a copy of this model is trained on some representatives from the buffer, and we called
this model Mbuffer (or NNrb). With these two models, we perform a scalar product in order to view if
there are similar, to know if the knowledge doesn‚Äôt disappear.With a negative scalar product, we keep
the current task, otherwise we make a projection of the Mtask model on Mbuffer. So this time buffer
is used for evaluating and correct the update current model. To compare with ER model, we have:</p>
<ul>
<li>
<p>Constraint when updating the parameters of the Mtask, mitigating catastrophic forgetting.</p>
</li>
<li>
<p>No proper rehearsal but a rehearsal buffer is still needed to obtain Mbuffer.</p>
</li>
</ul>
<p><img loading=lazy src=/Sirius14_Blog/img/article/cl_4.png alt=Agem>
<em>Figure 4: Agem schema</em></p>
<h3 id=hindsight-anchor-learning-halhttpsarxivorgpdf200208165pdf>Hindsight Anchor Learning (<a href=https://arxiv.org/pdf/2002.08165.pdf>HAL</a>)<a hidden class=anchor aria-hidden=true href=#hindsight-anchor-learning-halhttpsarxivorgpdf200208165pdf>#</a></h3>
<p>This approach seems to be relatively good, resembling an enhancement of
ER. This algorithm is both rehearsal and regularization based, indeed we keep ER implementation
with the buffer, but we are adding a constraint during update of the model. We want to limit the
forgetting on historical data points representative of a class (called anchors).For each class of each
task, we have one anchor, it encodes points that would maximize the catastrophic forgetting. Thanks
to the anchors, the model cannot diverge too much using this method. These are used in the update
calculation and are calculated with at the end of each task. Creating anchors is linear because you
don‚Äôt need to re-calculate them, nevertheless all the anchors are used to update the model. If we
compare to ER contrary to A-Gem, we have also similarities:</p>
<ul>
<li>
<p>System of anchors differs to ER and bring the regularization method to this algorithm.</p>
</li>
<li>
<p>The buffer is still concatenated to the current task buffer so the same thing as ER.</p>
</li>
</ul>
<p><img loading=lazy src=/Sirius14_Blog/img/article/cl_5.png alt=Hal>
<em>Figure 5: Hal schema</em></p>
<h3 id=dark-experience-replay-and-improvement-derhttpsarxivorgpdf200407211pdfderhttpsarxivorgpdf220100766pdf>Dark Experience Replay and improvement (<a href=https://arxiv.org/pdf/2004.07211.pdf>DER</a>/<a href=https://arxiv.org/pdf/2201.00766.pdf>DER++</a>)<a hidden class=anchor aria-hidden=true href=#dark-experience-replay-and-improvement-derhttpsarxivorgpdf200407211pdfderhttpsarxivorgpdf220100766pdf>#</a></h3>
<p>Before explaining the how it works, I need to briefly explain what knowledge distillation is.</p>
<h4 id=knowledge-distillation-kd>Knowledge distillation (KD)<a hidden class=anchor aria-hidden=true href=#knowledge-distillation-kd>#</a></h4>
<p>KD is separated in two models, teacher and student. The student
model delivers predictions. There are 2 kinds of predictions:</p>
<ul>
<li>
<p>Soft predictions are a tensor of probabilities also called lofts, in fact the last layer of our model,
the step before we predict the class.</p>
</li>
<li>
<p>Hard predictions are the step after, the class predicted by our model.</p>
</li>
</ul>
<p><img loading=lazy src=/Sirius14_Blog/img/article/cl_6.png alt="Knowledge distillation">
<em>Figure 6: Knowledge distillation schema</em></p>
<p>Then we can calculate the loss of each one of these predictions, we combine distillation loss and student
loss into an only loss value using coefficient to regulate if we prefer to highlight one loss compared to
the other ones. As you can observe in Figure 6 we have a green circle and a red circle, this means that
DER algorithm only used distillation loss and not combine to student loss in green. In red we have
DER++ which add student loss.</p>
<h4 id=derhttpsarxivorgpdf200407211pdf><a href=https://arxiv.org/pdf/2004.07211.pdf>DER</a><a hidden class=anchor aria-hidden=true href=#derhttpsarxivorgpdf200407211pdf>#</a></h4>
<p>This algorithm is rehearsal and regularization based. It complements rehearsal with an
additional knowledge distillation objective, to the difference between ER is that we store logits (soft
labels) alongside its representative in the buffer instead of relying on a single class output (hard label).
Thus, we need to modify the actual buffer implementation (which is the buffer‚Äôs one). Regularization
aspect is brought by distillation loss that we add to current mini batch loss. We still have difference
with an ER model like:</p>
<ul>
<li>
<p>Representatives are stored alongside their logits (activations of last layer).</p>
</li>
<li>
<p>Evaluate the model on both older logits and current ones (knowledge distillation).</p>
</li>
</ul>
<p><img loading=lazy src=/Sirius14_Blog/img/article/cl_7.png alt=Der>
<em>Figure 7: Der schema</em></p>
<h4 id=derhttpsarxivorgpdf220100766pdf><a href=https://arxiv.org/pdf/2201.00766.pdf>DER++</a><a hidden class=anchor aria-hidden=true href=#derhttpsarxivorgpdf220100766pdf>#</a></h4>
<p>DER++ This is an improvement of DER, this algorithm is still rehearsal and regularization
based, in a nutshell, ER uses hard labels only (class outputs), DER uses soft labels only (logits).
DER++ uses both. Conceptually nothing changes between DER and DER++ except the buffer
implementation as a matter of the fact we need to store labels and logits, so in contrary of DER we
store an additional component, which leave us less size for representatives.</p>
<p><img loading=lazy src=/Sirius14_Blog/img/article/cl_8.png alt=Der++>
<em>Figure 8: Der++ schema</em></p>
<h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2>
<p>Continual Learning algorithms concept is still young and Machine learning algorithms continue to outperform them in terms of performance. However usage is not the same, and may be in 10 years catastrophic forgetting will no longer be an issue. Then CL could be very useful in our daily lives. This area of research is vast and each year new algorithms arrive, but we are still in the early
stages. Over this report I explore the implementation and deployment part of a distributed testbed.
The impact of these presented algorithms is still underperforming against DL but with parallelisation
of calculations and a learning process which don‚Äôt incrementally this type of machine learning could
have a real impact on domain like medicine, finance, or autonomous car. With a faster training phase
and incremental learning, it‚Äôs also cheaper, and we can create new data in real time to improve the
model without training from scratch.</p>
<h2 id=source>Source<a hidden class=anchor aria-hidden=true href=#source>#</a></h2>
<p>Every explaination of this article was based on research paper and represent the culmination of an internship I had the opportunity to undertake. The result I obtained as well as implementation are not available in this blog. If you want more info you can DM me in Discord.</p>
<p>ER : <a href=https://arxiv.org/pdf/1811.11682.pdf>https://arxiv.org/pdf/1811.11682.pdf</a></p>
<p>A-GEM : <a href=https://arxiv.org/pdf/1812.00420.pdf>https://arxiv.org/pdf/1812.00420.pdf</a></p>
<p>HAL : <a href=https://arxiv.org/pdf/2002.08165.pdf>https://arxiv.org/pdf/2002.08165.pdf</a></p>
<p>DER : <a href=https://arxiv.org/pdf/2004.07211.pdf>https://arxiv.org/pdf/2004.07211.pdf</a></p>
<p>DER ++ : <a href=https://arxiv.org/pdf/2201.00766.pdf>https://arxiv.org/pdf/2201.00766.pdf</a></p>
<p>Continual Learning Pytorch Framework : <a href=https://github.com/aimagelab/mammoth>https://github.com/aimagelab/mammoth</a></p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://xelon-coder.github.io/Sirius14_Blog/tags/ai/>AI</a></li>
<li><a href=https://xelon-coder.github.io/Sirius14_Blog/tags/article/>Article</a></li>
<li><a href=https://xelon-coder.github.io/Sirius14_Blog/tags/pytorch/>Pytorch</a></li>
<li><a href=https://xelon-coder.github.io/Sirius14_Blog/tags/python/>Python</a></li>
<li><a href=https://xelon-coder.github.io/Sirius14_Blog/tags/continual-learning/>Continual Learning</a></li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2025 <a href=https://xelon-coder.github.io/Sirius14_Blog/>Sirius14 Blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>